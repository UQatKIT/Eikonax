{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Eikonax","text":""},{"location":"#installation","title":"Installation","text":""},{"location":"#overview-of-resources","title":"Overview of Resources","text":""},{"location":"#acknowledgement","title":"Acknowledgement","text":""},{"location":"api/core/","title":"Core Functions","text":""},{"location":"api/core/#eikonax.corefunctions","title":"corefunctions","text":"<p>Core functions for Eikonax forward solves and parametric derivatives.</p> <p>This module contains atomic functions that make up the Eikonax solver routines. They (and their automatic derivatives from JAX) are further used to evaluate parametric derivatives.</p> <p>Classes:</p> Name Description <code>MeshData</code> <p>Data characterizing a computational mesh from a vertex-centered perspective.</p> <code>InitialSites</code> <p>Initial site info.</p> <p>Functions:</p> Name Description <code>compute_softmin</code> <p>Numerically stable computation of the softmin function based on the Boltzmann operator.</p> <code>compute_softminmax</code> <p>Smooth double ReLU-type approximation that restricts a variable to the interval [0, 1].</p> <code>compute_edges</code> <p>Compute the edged of a triangle from vertex indices and coordinates.</p> <code>compute_optimal_update_parameters_soft</code> <p>Compute position parameter for update of a node within a specific triangle.</p> <code>compute_optimal_update_parameters_hard</code> <p>Compute position parameter for update of a node within a specific triangle.</p> <code>_compute_optimal_update_parameters</code> <p>Compute the optimal update parameter for the solution of the Eikonal equation.</p> <code>compute_fixed_update</code> <p>Compute update for a given vertex, triangle, and update parameter.</p> <code>compute_update_candidates_from_adjacent_simplex</code> <p>Compute all possible update candidates from an adjacent triangle.</p> <code>compute_vertex_update_candidates</code> <p>Compute all update candidates for a given vertex.</p> <code>grad_softmin</code> <p>The gradient of the softmin function requires further masking of infeasible values.</p>"},{"location":"api/core/#eikonax.corefunctions.grad_update_solution","title":"eikonax.corefunctions.grad_update_solution  <code>module-attribute</code>","text":"<pre><code>grad_update_solution = jax.grad(compute_fixed_update, argnums=0)\n</code></pre>"},{"location":"api/core/#eikonax.corefunctions.grad_update_parameter","title":"eikonax.corefunctions.grad_update_parameter  <code>module-attribute</code>","text":"<pre><code>grad_update_parameter = jax.grad(compute_fixed_update, argnums=1)\n</code></pre>"},{"location":"api/core/#eikonax.corefunctions.grad_update_lambda","title":"eikonax.corefunctions.grad_update_lambda  <code>module-attribute</code>","text":"<pre><code>grad_update_lambda = jax.grad(compute_fixed_update, argnums=2)\n</code></pre>"},{"location":"api/core/#eikonax.corefunctions.jac_lambda_soft_solution","title":"eikonax.corefunctions.jac_lambda_soft_solution  <code>module-attribute</code>","text":"<pre><code>jac_lambda_soft_solution = jax.jacobian(compute_optimal_update_parameters_soft, argnums=0)\n</code></pre>"},{"location":"api/core/#eikonax.corefunctions.jac_lambda_hard_solution","title":"eikonax.corefunctions.jac_lambda_hard_solution  <code>module-attribute</code>","text":"<pre><code>jac_lambda_hard_solution = jax.jacobian(compute_optimal_update_parameters_hard, argnums=0)\n</code></pre>"},{"location":"api/core/#eikonax.corefunctions.jac_lambda_soft_parameter","title":"eikonax.corefunctions.jac_lambda_soft_parameter  <code>module-attribute</code>","text":"<pre><code>jac_lambda_soft_parameter = jax.jacobian(compute_optimal_update_parameters_soft, argnums=1)\n</code></pre>"},{"location":"api/core/#eikonax.corefunctions.jac_lambda_hard_parameter","title":"eikonax.corefunctions.jac_lambda_hard_parameter  <code>module-attribute</code>","text":"<pre><code>jac_lambda_hard_parameter = jax.jacobian(compute_optimal_update_parameters_hard, argnums=1)\n</code></pre>"},{"location":"api/core/#eikonax.corefunctions.MeshData","title":"eikonax.corefunctions.MeshData  <code>dataclass</code>","text":"<p>Data characterizing a computational mesh from a vertex-centered perspective.</p> <p>Attributes:</p> Name Type Description <code>vertices</code> <code>jax.Array | npt.NDArray</code> <p>The coordinates of the vertices in the mesh. The dimension of this array is (num_vertices, dim), where num_vertices is the number of vertices in the mesh and dim is the dimension of the space in which the mesh is embedded.</p> <code>adjacency_data</code> <code>jax.Array | npt.NDArray</code> <p>Adjacency data for each vertex. This is the list of adjacent triangles, together with the two vertices that span the respective triangle with the current vertex. The dimension of this array is (num_vertices, max_num_adjacent_simplices, 4), where max_num_adjacent_simplices is the maximum number of simplices that are adjacent to a vertex in the mesh. All entries have this maximum size, as JAX only operates on homogeneous data structures. If a vertex has fewer than max_num_adjacent_simplices adjacent simplices, the remaining entries are filled with -1.</p>"},{"location":"api/core/#eikonax.corefunctions.MeshData.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Convert to jax arrays.</p>"},{"location":"api/core/#eikonax.corefunctions.InitialSites","title":"eikonax.corefunctions.InitialSites  <code>dataclass</code>","text":"<p>Initial site info.</p> <p>For a unique solution of the state-constrained Eikonal equation, the solution values need to be given a number of initial points (at least one). Multiple initial sites need to be compatible, in the sense that the arrival time from another source is not smaller than the initial value itself.</p> <p>Attributes:</p> Name Type Description <code>inds</code> <code>jax.Array | npt.NDArray</code> <p>The indices of the nodes where the initial sites are placed.</p> <code>values</code> <code>jax.Array | npt.NDArray</code> <p>The values of the initial sites.</p>"},{"location":"api/core/#eikonax.corefunctions.InitialSites.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Convert to jax arrays.</p>"},{"location":"api/core/#eikonax.corefunctions.compute_softmin","title":"eikonax.corefunctions.compute_softmin","text":"<pre><code>compute_softmin(\n    args: jtReal[jax.Array, ...], min_arg: jtReal[jax.Array, \"\"], order: int\n) -&gt; jtFloat[jax.Array, \"\"]\n</code></pre> <p>Numerically stable computation of the softmin function based on the Boltzmann operator.</p> <p>This softmin function is applied to actual minimum values, meaning it does not have a purpose on the evaluation level. It renders the minimum computation differentiable, however. Importantly, the Boltzmann softmin is value preserving, meaning that the solution of the eikonal equation is the same as for a hard minimum. As JAX works on homogeneous arrays only, non-minimal values are also passed to this function. They are assumed to be masked as jnp.inf, which are handled in a numerically stable way by this routine.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>jax.Array</code> <p>Values to compute the soft minimum over</p> required <code>min_arg</code> <code>jax.Array</code> <p>The actual value of the minimum argument, necessary for numerical stability</p> required <code>order</code> <code>int</code> <p>Approximation order of the softmin function</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, '']</code> <p>jax.Array: Soft minimum value</p>"},{"location":"api/core/#eikonax.corefunctions.compute_softminmax","title":"eikonax.corefunctions.compute_softminmax","text":"<pre><code>compute_softminmax(value: jtReal[jax.Array, ...], order: int) -&gt; jtFloat[jax.Array, ...]\n</code></pre> <p>Smooth double ReLU-type approximation that restricts a variable to the interval [0, 1].</p> <p>The method is numerically stable, obeys the value range, and does not introduce any new extrema.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>jax.Array</code> <p>variable to restrict to range [0,1]</p> required <code>order</code> <code>int</code> <p>Approximation order of the smooth approximation</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, ...]</code> <p>jax.Array: Smoothed/restricted value</p>"},{"location":"api/core/#eikonax.corefunctions.compute_edges","title":"eikonax.corefunctions.compute_edges","text":"<pre><code>compute_edges(\n    i: jtInt[jax.Array, \"\"],\n    j: jtInt[jax.Array, \"\"],\n    k: jtInt[jax.Array, \"\"],\n    vertices: jtFloat[jax.Array, \"num_vertices dim\"],\n) -&gt; tuple[jtFloat[jax.Array, dim], jtFloat[jax.Array, dim], jtFloat[jax.Array, dim]]\n</code></pre> <p>Compute the edged of a triangle from vertex indices and coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>jax.Array</code> <p>First vertex index of a triangle</p> required <code>j</code> <code>jax.Array</code> <p>Second vertex index of a triangle</p> required <code>k</code> <code>jax.Array</code> <p>Third vertex index of a triangle</p> required <code>vertices</code> <code>jax.Array</code> <p>jax.Array of all vertex coordinates</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, dim], jtFloat[jax.Array, dim], jtFloat[jax.Array, dim]]</code> <p>tuple[jax.Array, jax.Array, jax.Array]: Triangle edge vectors</p>"},{"location":"api/core/#eikonax.corefunctions.compute_optimal_update_parameters_soft","title":"eikonax.corefunctions.compute_optimal_update_parameters_soft","text":"<pre><code>compute_optimal_update_parameters_soft(\n    solution_values: jtFloat[jax.Array, 2],\n    parameter_tensor: jtFloat[jax.Array, \"dim dim\"],\n    edges: tuple[jtFloat[jax.Array, dim], jtFloat[jax.Array, dim], jtFloat[jax.Array, dim]],\n    softminmax_order: int,\n    softminmax_cutoff: Real,\n) -&gt; jtFloat[jax.Array, 4]\n</code></pre> <p>Compute position parameter for update of a node within a specific triangle.</p> <p>For a given vertex i and adjacent triangle, we compute the update for the solution of the Eikonal as propagating from a point on the connecting edge of the opposite vertices j and k. We thereby assume the solution value to vary linearly on that dge. The update parameter in [0,1] indicates the optimal linear combination of the solution values at j and k, in the sense that the solution value at i is minimized. As the underlying optimization problem is constrained, we compute the solutions of the unconstrained problem, as well as the boundary values. The former are constrained to the feasible region [0,1] by a soft minmax function. We further clip values lying to far outside the feasible region, by masking them with value -1. This function is a wrapper, for the unconstrained solution values, it calls the implementation function <code>_compute_optimal_update_parameters</code>.</p> <p>Parameters:</p> Name Type Description Default <code>solution_values</code> <code>jax.Array</code> <p>Current solution values, as per the previous iteration</p> required <code>parameter_tensor</code> <code>jax.Array</code> <p>Parameter tensor for the current triangle</p> required <code>edges</code> <code>tuple[jax.Array, jax.Array, jax.Array]</code> <p>Edge vectors of the triangle under consideration</p> required <code>softminmax_order</code> <code>int</code> <p>Order of the soft minmax function to be employed</p> required <code>softminmax_cutoff</code> <code>int</code> <p>Cutoff value beyond parameter values are considered infeasible and masked with -1</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 4]</code> <p>jax.Array: All possible candidates for the update parameter, according to the solution of the constrained optimization problem</p>"},{"location":"api/core/#eikonax.corefunctions.compute_optimal_update_parameters_hard","title":"eikonax.corefunctions.compute_optimal_update_parameters_hard","text":"<pre><code>compute_optimal_update_parameters_hard(\n    solution_values: jtFloat[jax.Array, 2],\n    parameter_tensor: jtFloat[jax.Array, \"dim dim\"],\n    edges: tuple[jtFloat[jax.Array, dim], jtFloat[jax.Array, dim], jtFloat[jax.Array, dim]],\n) -&gt; jtFloat[jax.Array, 4]\n</code></pre> <p>Compute position parameter for update of a node within a specific triangle.</p> <p>For a given vertex i and adjacent triangle, we compute the update for the solution of the Eikonal as propagating from a point on the connecting edge of the opposite vertices j and k. We thereby assume the solution value to vary linearly on that dge. The update parameter in [0,1] indicates the optimal linear combination of the solution values at j and k, in the sense that the solution value at i is minimized. As the underlying optimization problem is constrained, we compute the solutions of the unconstrained problem, as well as the boundary values. The former are constrained to the feasible region [0,1] by a simple cutoff. This function is a wrapper, for the unconstrained solution values, it calls the implementation function <code>_compute_optimal_update_parameters</code>.</p> <p>Parameters:</p> Name Type Description Default <code>solution_values</code> <code>jax.Array</code> <p>Current solution values, as per the previous iteration</p> required <code>parameter_tensor</code> <code>jax.Array</code> <p>Parameter tensor for the current triangle</p> required <code>edges</code> <code>tuple[jax.Array, jax.Array, jax.Array]</code> <p>Edge vectors of the triangle under consideration</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 4]</code> <p>jax.Array: All possible candidates for the update parameter, according to the solution of the constrained optimization problem</p>"},{"location":"api/core/#eikonax.corefunctions._compute_optimal_update_parameters","title":"eikonax.corefunctions._compute_optimal_update_parameters","text":"<pre><code>_compute_optimal_update_parameters(\n    solution_values: jtFloat[jax.Array, 2],\n    parameter_tensor: jtFloat[jax.Array, \"dim dim\"],\n    edges: tuple[jtFloat[jax.Array, dim], jtFloat[jax.Array, dim], jtFloat[jax.Array, dim]],\n) -&gt; tuple[jtFloat[jax.Array, \"\"], jtFloat[jax.Array, \"\"]]\n</code></pre> <p>Compute the optimal update parameter for the solution of the Eikonal equation.</p> <p>The function works for the update within a given triangle. The solutions of the unconstrained minimization problem are given as the roots of a quadratic polynomial. They may or may not lie inside the feasible region [0,1]. The function returns both solutions, which are then further processed in the calling wrapper.</p>"},{"location":"api/core/#eikonax.corefunctions.compute_fixed_update","title":"eikonax.corefunctions.compute_fixed_update","text":"<pre><code>compute_fixed_update(\n    solution_values: jtFloat[jax.Array, 2],\n    parameter_tensor: jtFloat[jax.Array, \"dim dim\"],\n    lambda_value: jtFloat[jax.Array, \"\"],\n    edges: tuple[jtFloat[jax.Array, dim], jtFloat[jax.Array, dim], jtFloat[jax.Array, dim]],\n) -&gt; jtFloat[jax.Array, \"\"]\n</code></pre> <p>Compute update for a given vertex, triangle, and update parameter.</p> <p>The update value is given by the solution at a point  on the edge between the opposite vertices, plus the travel time from that point to the vertices under consideration.</p> <p>Parameters:</p> Name Type Description Default <code>solution_values</code> <code>jax.Array</code> <p>Current solution values at opposite vertices j and k, as per the previous iteration</p> required <code>parameter_tensor</code> <code>jax.Array</code> <p>Conductivity tensor for the current triangle</p> required <code>lambda_value</code> <code>jax.Array</code> <p>Optimal update parameter</p> required <code>edges</code> <code>tuple[jax.Array, jax.Array, jax.Array]</code> <p>Edge vectors of the triangle under consideration</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, '']</code> <p>jax.Array: Updated solution value for the vertex under consideration</p>"},{"location":"api/core/#eikonax.corefunctions.compute_update_candidates_from_adjacent_simplex","title":"eikonax.corefunctions.compute_update_candidates_from_adjacent_simplex","text":"<pre><code>compute_update_candidates_from_adjacent_simplex(\n    old_solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_simplices dim dim\"],\n    adjacency_data: jtInt[jax.Array, max_num_adjacent_simplices],\n    vertices: jtFloat[jax.Array, \"num_vertices dim\"],\n    use_soft_update: bool,\n    softminmax_order: int | None,\n    softminmax_cutoff: Real | None,\n) -&gt; tuple[jtFloat[jax.Array, 4], jtFloat[jax.Array, 4]]\n</code></pre> <p>Compute all possible update candidates from an adjacent triangle.</p> <p>Given a vertex and an adjacent triangle, this method computes all optimal update parameter candidates and the corresponding update values. To obey JAX's homogeneous array requirement, update values are also computed for infeasible parameter values, and have to be masked in the calling routine. This methods basically collects all results from the <code>compute_optimal_update_parameters</code> and <code>compute_fixed_update</code> methods.</p> <p>Parameters:</p> Name Type Description Default <code>old_solution_vector</code> <code>jax.Array</code> <p>Given solution vector, as per a previous iteration</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Array of all tensor fields</p> required <code>adjacency_data</code> <code>jax.Array</code> <p>Index of one adjaccent triangle and corresponding vertices</p> required <code>vertices</code> <code>jax.Array</code> <p>Array of all vertex coordinates</p> required <code>use_soft_update</code> <code>bool</code> <p>Flag to indicate whether to use a soft update or a hard update</p> required <code>softminmax_order</code> <code>int | None</code> <p>Order of the soft minmax function for the update parameter, see <code>compute_softminmax</code>. Only required for <code>use_soft_update=True</code></p> required <code>softminmax_cutoff</code> <code>Real | None</code> <p>Cutoff value for the soft minmax computation, see <code>compute_optimal_update_parameters</code>. Only required for <code>use_soft_update=True</code></p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, 4], jtFloat[jax.Array, 4]]</code> <p>tuple[jax.Array, jax.Array]: Update values and parameter candidates from the given triangle</p>"},{"location":"api/core/#eikonax.corefunctions.compute_vertex_update_candidates","title":"eikonax.corefunctions.compute_vertex_update_candidates","text":"<pre><code>compute_vertex_update_candidates(\n    old_solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_simplices dim dim\"],\n    adjacency_data: jtInt[jax.Array, \"max_num_adjacent_simplices 4\"],\n    vertices: jtFloat[jax.Array, \"num_vertices dim\"],\n    use_soft_update: bool,\n    softminmax_order: int,\n    softminmax_cutoff: Real,\n) -&gt; jtFloat[jax.Array, \"max_num_adjacent_simplices 4\"]\n</code></pre> <p>Compute all update candidates for a given vertex.</p> <p>This method combines all updates from adjacent triangles to a given vertex, as computed in the function <code>compute_update_candidates_from_adjacent_simplex</code>. Infeasible candidates are masked with jnp.inf.</p> <p>Parameters:</p> Name Type Description Default <code>old_solution_vector</code> <code>jax.Array</code> <p>Given solution vector, as per a previous iteration</p> required <code>tensor_field</code> <code>jax.Array</code> <p>jax.Array of all tensor fields</p> required <code>adjacency_data</code> <code>jax.Array</code> <p>Data of all adjacent triangles and corresponding vertices</p> required <code>vertices</code> <code>jax.Array</code> <p>jax.Array of all vertex coordinates</p> required <code>use_soft_update</code> <code>bool</code> <p>Flag to indicate whether to use a soft update or a hard update</p> required <code>softminmax_order</code> <code>int | None</code> <p>Order of the soft minmax function for the update parameter, see <code>compute_softminmax</code>. Only required for <code>use_soft_update=True</code></p> required <code>softminmax_cutoff</code> <code>Real | None</code> <p>Cutoff value for the soft minmax computation, see <code>compute_optimal_update_parameters</code>. Only required for <code>use_soft_update=True</code></p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'max_num_adjacent_simplices 4']</code> <p>jax.Array: All possible update values for the given vertex, infeasible vertices are masked with jnp.inf</p>"},{"location":"api/core/#eikonax.corefunctions.grad_softmin","title":"eikonax.corefunctions.grad_softmin","text":"<pre><code>grad_softmin(\n    args: jtFloat[jax.Array, num_args], min_arg: jtFloat[jax.Array, \"\"], _order: int\n) -&gt; jtFloat[jax.Array, num_args]\n</code></pre> <p>The gradient of the softmin function requires further masking of infeasible values.</p> <p>NOTE: this is only the gradient of the softmin function for identical, minimal values!</p>"},{"location":"api/derivator/","title":"Parametric Derivatives","text":""},{"location":"api/derivator/#eikonax.derivator","title":"derivator","text":"<p>Components for computing derivatives of the Eikonax solver.</p> <p>This module contains two main components. Firstly, the <code>PartialDerivator</code> evaluates the partial derivatives of the global Eikonax update operator w.r.t. the parameter field and the corresponding solution vector obtained from a forward solve. The <code>DerivativeSolver</code> component makes use of the fixed point/adjoint property of the Eikonax solver to evaluate total parametric derivatives.</p> <p>Classes:</p> Name Description <code>PartialDerivatorData</code> <p>Settings for initialization of partial derivator</p> <code>PartialDerivator</code> <p>Component for computing partial derivatives of the Godunov Update operator</p> <code>DerivativeSolver</code> <p>Main component for obtaining gradients from partial derivatives</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivatorData","title":"eikonax.derivator.PartialDerivatorData  <code>dataclass</code>","text":"<p>Settings for initialization of partial derivator.</p> <p>Attributes:</p> Name Type Description <code>softmin_order</code> <code>int</code> <p>Order of the softmin function applied to update candidates with identical, minimal arrival times.</p> <code>softminmax_order</code> <code>int</code> <p>Order of the the soft minmax function for differentiable transformation of the update parameters</p> <code>softminmax_cutoff</code> <code>Real</code> <p>Cut-off in for minmax transformation, beyond which zero sensitivity is assumed.</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator","title":"eikonax.derivator.PartialDerivator","text":"<p>               Bases: <code>eqx.Module</code></p> <p>Component for computing partial derivatives of the Godunov Update operator.</p> <p>Methods:</p> Name Description <code>compute_partial_derivatives</code> <p>Compute the partial derivatives of the Godunov update operator with respect to the solution vector and the parameter tensor field, given a state for both variables</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator.__init__","title":"__init__","text":"<pre><code>__init__(\n    mesh_data: corefunctions.MeshData,\n    derivator_data: PartialDerivatorData,\n    initial_sites: corefunctions.InitialSites,\n) -&gt; None\n</code></pre> <p>Constructor for the partial derivator object.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_data</code> <code>corefunctions.MeshData</code> <p>Mesh data object also utilized for the Eikonax solver, contains adjacency data for every vertex.</p> required <code>derivator_data</code> <code>PartialDerivatorData</code> <p>Settings for initialization of the derivator.</p> required <code>initial_sites</code> <code>corefunctions.InitialSites</code> <p>Locations and values at source points</p> required"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator.compute_partial_derivatives","title":"compute_partial_derivatives","text":"<pre><code>compute_partial_derivatives(\n    solution_vector: jtFloat[jax.Array | npt.NDArray, num_vertices],\n    tensor_field: jtFloat[jax.Array | npt.NDArray, \"num_simplices dim dim\"],\n) -&gt; tuple[\n    tuple[\n        jtInt[jax.Array, num_sol_values],\n        jtInt[jax.Array, num_sol_values],\n        jtFloat[jax.Array, num_sol_values],\n    ],\n    tuple[\n        jtInt[jax.Array, num_param_values],\n        jtInt[jax.Array, num_param_values],\n        jtFloat[jax.Array, \"num_param_values dim dim\"],\n    ],\n]\n</code></pre> <p>Compute the partial derivatives of the Godunov update operator.</p> <p>This method provided the main interface for computing the partial derivatives of the global Eikonax update operator with respect to the solution vector and the parameter tensor field. The updates are computed locally for each vertex, such that the resulting data structures are sparse. Subsequently, further zero entries are removed to reduce the memory footprint. The derivatives computed in this component can be utilized to compute the total parametric derivative via a fix point equation, given that the provided solution vector is that fix point. The computation of partial derivatives is possible with a single pass over the mesh, since the solution of the Eikonax equation, and therefore causality within the Godunov update scheme, is known.</p> <p>Parameters:</p> Name Type Description Default <code>solution_vector</code> <code>jax.Array</code> <p>Current solution</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Parameter field</p> required <p>Returns:</p> Type Description <code>tuple[tuple[jtInt[jax.Array, num_sol_values], jtInt[jax.Array, num_sol_values], jtFloat[jax.Array, num_sol_values]], tuple[jtInt[jax.Array, num_param_values], jtInt[jax.Array, num_param_values], jtFloat[jax.Array, 'num_param_values dim dim']]]</code> <p>tuple[tuple[jax.Array, jax.Array, jax.Array], tuple[jax.Array, jax.Array, jax.Array]]: Partial derivatives with respect to the solution vector and the parameter tensor field. Both quantities are given as arrays over all local contributions, making them sparse in the global context.</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._compress_partial_derivative_solution","title":"_compress_partial_derivative_solution","text":"<pre><code>_compress_partial_derivative_solution(\n    partial_derivative_solution: jtFloat[jax.Array, \"num_vertices max_num_adjacent_simplices 2\"],\n) -&gt; tuple[\n    jtInt[jax.Array, num_sol_values],\n    jtInt[jax.Array, num_sol_values],\n    jtFloat[jax.Array, num_sol_values],\n]\n</code></pre> <p>Compress the partial derivative data with respect to the solution vector.</p> <p>Compression consists of two steps:</p> <ol> <li>Remove zero entries in the sensitivity vector</li> <li>Set the sensitivity vector to zero at the initial sites, but keep them for later    computations.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>partial_derivative_solution</code> <code>jax.Array</code> <p>Raw data from partial derivative computation, with shape (N, num_adjacent_simplices, 2), N depends on the number of identical update paths for the vertices in the mesh.</p> required <p>Returns:</p> Type Description <code>jtInt[jax.Array, num_sol_values]</code> <p>tuple[jax.Array, jax.Array, jax.Array]: Compressed data, represented as rows,</p> <code>jtInt[jax.Array, num_sol_values]</code> <p>columns and values for initialization in sparse matrix. Shape depends on number of non-zero entries</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._compress_partial_derivative_parameter","title":"_compress_partial_derivative_parameter","text":"<pre><code>_compress_partial_derivative_parameter(\n    partial_derivative_parameter: jtFloat[\n        jax.Array, \"num_vertices max_num_adjacent_simplices dim dim\"\n    ],\n) -&gt; tuple[\n    jtInt[jax.Array, num_param_values],\n    jtInt[jax.Array, num_param_values],\n    jtFloat[jax.Array, \"num_param_values dim dim\"],\n]\n</code></pre> <p>Compress the partial derivative data with respect to the parameter field.</p> <p>Compression consists of two steps:</p> <ol> <li>Remove tensor components from the sensitivity data, if all entries are zero</li> <li>Set the sensitivity vector to zero at the initial sites, but keep them for later    computations.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>partial_derivative_parameter</code> <code>jax.Array</code> <p>Raw data from partial derivative computation, with shape (N, num_adjacent_simplices, dim, dim), N depends on the number of identical update paths for the vertices in the mesh.</p> required <p>Returns:</p> Type Description <code>tuple[jtInt[jax.Array, num_param_values], jtInt[jax.Array, num_param_values], jtFloat[jax.Array, 'num_param_values dim dim']]</code> <p>tuple[jax.Array, jax.Array, jax.Array]: Compressed data, represented as rows, columns and values to be further processes for sparse matrix assembly. Shape depends on number of non-zero entries</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._compute_global_partial_derivatives","title":"_compute_global_partial_derivatives","text":"<pre><code>_compute_global_partial_derivatives(\n    solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_simplices dim dim\"],\n) -&gt; tuple[\n    jtFloat[jax.Array, \"num_vertices max_num_adjacent_simplices 2\"],\n    jtFloat[jax.Array, \"num_vertices max_num_adjacent_simplices dim dim\"],\n]\n</code></pre> <p>Compute partial derivatives of the global update operator.</p> <p>The method is a jitted and vectorized call to the <code>_compute_vertex_partial_derivative</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>solution_vector</code> <code>jax.Array</code> <p>Global solution vector</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Global parameter tensor field</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, 'num_vertices max_num_adjacent_simplices 2'], jtFloat[jax.Array, 'num_vertices max_num_adjacent_simplices dim dim']]</code> <p>tuple[jax.Array, jax.Array]: Raw data for partial derivatives, with shapes (N, num_adjacent_simplices, 2) and (N, num_adjacent_simplices, dim, dim), N depends on the number of identical update paths for the vertices in the mesh.</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._compute_vertex_partial_derivatives","title":"_compute_vertex_partial_derivatives","text":"<pre><code>_compute_vertex_partial_derivatives(\n    solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_simplices dim dim\"],\n    adjacency_data: jtInt[jax.Array, \"max_num_adjacent_simplices 4\"],\n) -&gt; tuple[\n    jtFloat[jax.Array, \"max_num_adjacent_simplices 2\"],\n    jtFloat[jax.Array, \"max_num_adjacent_simplices dim dim\"],\n]\n</code></pre> <p>Compute partial derivatives for the update of a single vertex.</p> <p>The method computes candidates for all respective subterms through calls to further methods. These candidates are filtered for feyasibility by means of JAX filters. The sofmin function (and its gradient) is applied to the directions of all optimal updates to ensure differentiability, other contributions are discarded. Lasty, the evaluated contributions are combined according to the form of the \"total differential\" for the parrtial derivatives.</p> <p>Parameters:</p> Name Type Description Default <code>solution_vector</code> <code>jax.Array</code> <p>Global solution vector</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Global parameter tensor field</p> required <code>adjacency_data</code> <code>jax.Array</code> <p>Adjacency data for the vertex under consideration</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, 'max_num_adjacent_simplices 2'], jtFloat[jax.Array, 'max_num_adjacent_simplices dim dim']]</code> <p>tuple[jax.Array, jax.Array]: Partial derivatives for the given vertex</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._compute_vertex_partial_derivative_candidates","title":"_compute_vertex_partial_derivative_candidates","text":"<pre><code>_compute_vertex_partial_derivative_candidates(\n    solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_simplices dim dim\"],\n    adjacency_data: jtInt[jax.Array, \"max_num_adjacent_simplices 4\"],\n) -&gt; tuple[\n    jtFloat[jax.Array, \"max_num_adjacent_simplices 4 2\"],\n    jtFloat[jax.Array, \"max_num_adjacent_simplices 4 dim dim\"],\n]\n</code></pre> <p>Compute partial derivatives corresponding to potential update candidates for a vertex.</p> <p>Update candidates and corresponding derivatives are computed for all adjacent simplices, and for all possible update parameters per simplex.</p> <p>Parameters:</p> Name Type Description Default <code>solution_vector</code> <code>jax.Array</code> <p>Global solution vector</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Global parameter field</p> required <code>adjacency_data</code> <code>jax.Array</code> <p>Adjacency data for the given vertex</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, 'max_num_adjacent_simplices 4 2'], jtFloat[jax.Array, 'max_num_adjacent_simplices 4 dim dim']]</code> <p>tuple[jax.Array, jax.Array]: Candidates for partial derivatives</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._compute_partial_derivative_candidates_from_adjacent_simplex","title":"_compute_partial_derivative_candidates_from_adjacent_simplex","text":"<pre><code>_compute_partial_derivative_candidates_from_adjacent_simplex(\n    solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_simplices dim dim\"],\n    adjacency_data: jtInt[jax.Array, 4],\n) -&gt; tuple[jtFloat[jax.Array, \"4 2\"], jtFloat[jax.Array, \"4 dim dim\"]]\n</code></pre> <p>Compute partial derivatives for all update candidates within an adjacent simplex.</p> <p>The update candidates are evaluated according to the different candidates for the optimization parameters lambda. Contributions are combined to the form of the involved total differentials.</p> <p>Parameters:</p> Name Type Description Default <code>solution_vector</code> <code>jax.Array</code> <p>Global solution vector</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Flobal parameter field</p> required <code>adjacency_data</code> <code>jax.Array</code> <p>Adjacency data for the given vertex and simplex</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, '4 2'], jtFloat[jax.Array, '4 dim dim']]</code> <p>tuple[jax.Array, jax.Array]: Derivative candidate from the given simplex</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._filter_candidates","title":"_filter_candidates  <code>staticmethod</code>","text":"<pre><code>_filter_candidates(\n    vertex_update_candidates: jtFloat[jax.Array, \"max_num_adjacent_simplices 4\"],\n    grad_update_solution_candidates: jtFloat[jax.Array, \"max_num_adjacent_simplices 4 2\"],\n    grad_update_parameter_candidates: jtFloat[jax.Array, \"max_num_adjacent_simplices 4 dim dim\"],\n) -&gt; tuple[\n    jtFloat[jax.Array, \"\"],\n    jtFloat[jax.Array, \"max_num_adjacent_simplices 4 2\"],\n    jtFloat[jax.Array, \"max_num_adjacent_simplices 4 dim dim\"],\n]\n</code></pre> <p>Mask irrelevant derivative candidates so that they are discarded later.</p> <p>Values are masked by setting them to zero or infinity, depending on the routine in which they are utilized later. Partial derivatives are only relevant if the corresponding update corresponds to an optimal path.</p> <p>Parameters:</p> Name Type Description Default <code>vertex_update_candidates</code> <code>jax.Array</code> <p>Update candidates for a given vertex</p> required <code>grad_update_solution_candidates</code> <code>jax.Array</code> <p>Partial derivative candidates w.r.t. the solution vector</p> required <code>grad_update_parameter_candidates</code> <code>jax.Array</code> <p>Partial derivative candidates w.r.t. the parameter field</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, ''], jtFloat[jax.Array, 'max_num_adjacent_simplices 4 2'], jtFloat[jax.Array, 'max_num_adjacent_simplices 4 dim dim']]</code> <p>tuple[jax.Array, jax.Array, jax.Array]: Optimal update value, masked partial derivatives</p>"},{"location":"api/derivator/#eikonax.derivator.PartialDerivator._compute_lambda_grad","title":"_compute_lambda_grad","text":"<pre><code>_compute_lambda_grad(\n    solution_values: jtFloat[jax.Array, 2],\n    parameter_tensor: jtFloat[jax.Array, \"dim dim\"],\n    edges: tuple[jtFloat[jax.Array, dim], jtFloat[jax.Array, dim], jtFloat[jax.Array, dim]],\n) -&gt; tuple[jtFloat[jax.Array, \"4 2\"], jtFloat[jax.Array, \"4 dim dim\"]]\n</code></pre> <p>Compute the partial derivatives of update parameters for a single vertex.</p> <p>This method evaluates the partial derivatives of the update parameters with respect to the current solution vector and the given parameter field, for a single triangle.</p> <p>Parameters:</p> Name Type Description Default <code>solution_values</code> <code>jax.Array</code> <p>Current solution values at the opposite vertices of the considered triangle</p> required <code>parameter_tensor</code> <code>jax.Array</code> <p>Parameter tensor for the given triangle</p> required <code>edges</code> <code>tuple[jax.Array, jax.Array, jax.Array]</code> <p>Edges of the considered triangle</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, '4 2'], jtFloat[jax.Array, '4 dim dim']]</code> <p>tuple[jax.Array, jax.Array]: Jacobians of the update parameters w.r.t. the solution vector and the parameter tensor</p>"},{"location":"api/derivator/#eikonax.derivator.DerivativeSolver","title":"eikonax.derivator.DerivativeSolver","text":"<p>Main component for obtaining gradients from partial derivatives.</p> <p>The Eikonax derivator computes partial derivatives of the global update operator with respect to the solution vector and the parameter tensor field. At the fixed point of the iterative update scheme, meaning the correct solution according to the discrete upwind scheme, these parrial derivatives can be used to obtain the total Jacobian of the global update operator with respect to the parameter tensor field. In practice, we are typically concerned with the parametric gradient of a cost functional that comprises the solution vector. The partial differential equation, which connects solution vector and parameter field, acts as a constraint in this context. The partial derivator computes the the so-called adjoint variable in this context, by solving a linear system of equation. The system matrix is obtained from the partial derivative of the global update operator with respect to the solution vector. Importantly, we can order the indices of the solution vector according to the size of the respective solution values. Because the update operator obeys upwind causality, the system matrix becomes triangular under such a permutation, and we can solve the linear system through simple back-substitution. In the context of an optimization problem, the right-hand-side is given as the partial derivative of the cost functional with respect to the solution vector.</p> <p>Methods:</p> Name Description <code>solve</code> <p>Solve the linear system for the adjoint variable</p>"},{"location":"api/derivator/#eikonax.derivator.DerivativeSolver.sparse_system_matrix","title":"sparse_system_matrix  <code>property</code>","text":"<pre><code>sparse_system_matrix: sp.sparse.csc_matrix\n</code></pre> <p>Get system matrix.</p>"},{"location":"api/derivator/#eikonax.derivator.DerivativeSolver.sparse_permutation_matrix","title":"sparse_permutation_matrix  <code>property</code>","text":"<pre><code>sparse_permutation_matrix: sp.sparse.csc_matrix\n</code></pre> <p>Get permutation matrix.</p>"},{"location":"api/derivator/#eikonax.derivator.DerivativeSolver.__init__","title":"__init__","text":"<pre><code>__init__(\n    solution: jtFloat[jax.Array | npt.NDArray, num_vertices],\n    sparse_partial_update_solution: tuple[\n        jtInt[jax.Array, num_sol_values],\n        jtInt[jax.Array, num_sol_values],\n        jtFloat[jax.Array, num_sol_values],\n    ],\n) -&gt; None\n</code></pre> <p>Constructor for the derivative solver.</p> <p>Initializes the causality-inspired permutation matrix, and afterwards the permuted system matrix, which is triangular.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>jax.Array | npt.NDArray</code> <p>Obtained solution of the Eikonal equation</p> required <code>sparse_partial_update_solution</code> <code>tuple[jax.Array, jax.Array, jax.Array]</code> <p>Sparse representation of the partial derivative G_u, containing row inds, column inds and values. These structures might contain redundances, which are automatically removed through summation in the sparse matrix assembly later.</p> required"},{"location":"api/derivator/#eikonax.derivator.DerivativeSolver.solve","title":"solve","text":"<pre><code>solve(\n    right_hand_side: jtFloat[jax.Array | npt.NDArray, num_vertices],\n) -&gt; jtFloat[npt.NDArray, num_parameters]\n</code></pre> <p>Solve the linear system for the parametric gradient.</p> <p>The right-hand-side needs to be given as the partial derivative of the prescribed cost functional w.r.t. the solution vector. This right-hand-side is permutated according to the causality of the solution. Subsequently, the linear system can be solved through (sparse) back-substitution. The solution is then permutated back to the original ordering.</p> <p>Parameters:</p> Name Type Description Default <code>right_hand_side</code> <code>jax.Array | npt.NDArray</code> <p>RHS for the linear system solve</p> required <p>Returns:</p> Type Description <code>jtFloat[npt.NDArray, num_parameters]</code> <p>np.ndarray: Solution of the linear system solve, corresponding to the adjoint in an optimization context.</p>"},{"location":"api/derivator/#eikonax.derivator.DerivativeSolver._assemble_permutation_matrix","title":"_assemble_permutation_matrix","text":"<pre><code>_assemble_permutation_matrix(solution: jtFloat[npt.NDArray, num_vertices]) -&gt; sp.sparse.csc_matrix\n</code></pre> <p>Construct permutation matrix for index ordering.</p> <p>From a given solution vector, we know from the properties of the upwind update scheme that causaility is given through the size of the respective solution values. This means that nodes with higher solution values are only influenced by nodes with lower solution values. With respect to linear system solves involving the global update operator, this means that we can obtain triangular matrices through an according permutation.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>npt.NDArray</code> <p>Obtained solution of the eikonal equation</p> required <p>Returns:</p> Type Description <code>sp.sparse.csc_matrix</code> <p>sp.sparse.csc_matrix: Sparse permutation matrix</p>"},{"location":"api/derivator/#eikonax.derivator.DerivativeSolver._assemble_system_matrix","title":"_assemble_system_matrix","text":"<pre><code>_assemble_system_matrix(\n    sparse_partial_update_solution: tuple[\n        jtInt[npt.NDArray, num_sol_values],\n        jtInt[npt.NDArray, num_sol_values],\n        jtFloat[npt.NDArray, num_sol_values],\n    ],\n    num_points: int,\n) -&gt; sp.sparse.csc_matrix\n</code></pre> <p>Assemble system matrix for gradient solver.</p> <p>The parametric gradient of the global update operator is obtained from a linear system solve, where the system matrix is given as \\((I-G_u)^T\\), where \\(G_u\\) is the partial derivative of the global update operator with respect to the solution vector. According to the causality of the solution, we can permutate the system matrix to a triangular form.</p> <p>Parameters:</p> Name Type Description Default <code>sparse_partial_update_solution</code> <code>tuple[npt.NDArray, npt.NDArray, npt.NDArray]</code> <p>Sparse representation of the partial derivative \\(G_u\\), containing row inds, column inds and values. These structures might contain redundances, which are automatically removed through summation in the sparse matrix assembly.</p> required <code>num_points</code> <code>int</code> <p>Number of mesh points</p> required <p>Returns:</p> Type Description <code>sp.sparse.csc_matrix</code> <p>sp.sparse.csc_matrix: Sparse representation of the permuted system matrix</p>"},{"location":"api/derivator/#eikonax.derivator.compute_eikonax_jacobian","title":"eikonax.derivator.compute_eikonax_jacobian","text":"<pre><code>compute_eikonax_jacobian(\n    derivative_solver: DerivativeSolver, partial_derivative_parameter: sp.sparse.coo_matrix\n) -&gt; npt.NDArray\n</code></pre> <p>Compute Jacobian from concatenation of gradients, computed with unit vector RHS.</p> <p>Warning</p> <p>This method should only be used for small problems.</p> <p>Parameters:</p> Name Type Description Default <code>derivative_solver</code> <code>DerivativeSolver</code> <p>Initialized derivative solver object</p> required <code>partial_derivative_parameter</code> <code>sp.sparse.coo_matrix</code> <p>Partial derivative of the global update operator with respect to the parameter tensor field</p> required <p>Returns:</p> Type Description <code>npt.NDArray</code> <p>npt.NDArray: (Dense) Jacobian matrix</p>"},{"location":"api/derivator/#eikonax.derivator.compute_eikonax_hessian","title":"eikonax.derivator.compute_eikonax_hessian","text":"<pre><code>compute_eikonax_hessian() -&gt; None\n</code></pre> <p>Compute Hessian matrix.</p> <p>Not implemented yet</p>"},{"location":"api/finitediff/","title":"Comparison with Finite Differences","text":""},{"location":"api/finitediff/#eikonax.finitediff","title":"finitediff","text":"<p>Finite difference approximation of parametric derivatives.</p> <p>Warning</p> <p>Finite difference approximations are typically computationally expensive and inaccurate. They should only be used for comparison in small test cases.</p> <p>Functions:</p> Name Description <code>finite_diff_1_forward</code> <p>Forward finite difference approximation of a first order derivative</p> <code>finite_diff_1_backward</code> <p>Backward finite difference approximation of a first order derivative</p> <code>finite_diff_1_central</code> <p>Central finite difference approximation of a first order derivative</p> <code>finite_diff_2</code> <p>Implement second order finite differences</p> <code>compute_fd_jacobian</code> <p>Compute the Jacobian of the Eikonal equation w.r.t. to parameter with finite differences</p>"},{"location":"api/finitediff/#eikonax.finitediff.finite_diff_1_forward","title":"eikonax.finitediff.finite_diff_1_forward","text":"<pre><code>finite_diff_1_forward(\n    func: Callable[[jtReal[npt.NDArray, M]], jtReal[npt.NDArray, M]],\n    eval_point: jtReal[npt.NDArray, M],\n    step_width: Real,\n    index: int,\n) -&gt; jtReal[npt.NDArray, N]\n</code></pre> <p>Forward finite difference approximation of a first order derivative.</p> <p>The function evaluates the partial derivative of the function <code>func</code> at the point <code>eval_point</code> for the index <code>index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Callable to use for FD computation</p> required <code>eval_point</code> <code>npt.NDArray</code> <p>Parameter value at which to approximate the derivative</p> required <code>step_width</code> <code>Real</code> <p>Step width of the finite difference</p> required <code>index</code> <code>int</code> <p>Vector component for which to compute partial derivative</p> required <p>Returns:</p> Type Description <code>jtReal[npt.NDArray, N]</code> <p>npt.NDArray: Partial derivative approximation</p>"},{"location":"api/finitediff/#eikonax.finitediff.finite_diff_1_backward","title":"eikonax.finitediff.finite_diff_1_backward","text":"<pre><code>finite_diff_1_backward(\n    func: Callable[[jtReal[npt.NDArray, M]], jtReal[npt.NDArray, M]],\n    eval_point: jtReal[npt.NDArray, M],\n    step_width: float,\n    index: int,\n) -&gt; jtReal[npt.NDArray, N]\n</code></pre> <p>Backward finite difference approximation of a first order derivative.</p> <p>The function evaluates the partial derivative of the function <code>func</code> at the point <code>eval_point</code> for the index <code>index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Callable to use for FD computation</p> required <code>eval_point</code> <code>npt.NDArray</code> <p>Parameter value at which to approximate the derivative</p> required <code>step_width</code> <code>Real</code> <p>Step width of the finite difference</p> required <code>index</code> <code>int</code> <p>Vector component for which to compute partial derivative</p> required <p>Returns:</p> Type Description <code>jtReal[npt.NDArray, N]</code> <p>npt.NDArray: Partial derivative approximation</p>"},{"location":"api/finitediff/#eikonax.finitediff.finite_diff_1_central","title":"eikonax.finitediff.finite_diff_1_central","text":"<pre><code>finite_diff_1_central(\n    func: Callable[[jtReal[npt.NDArray, M]], jtReal[npt.NDArray, M]],\n    eval_point: jtReal[npt.NDArray, M],\n    step_width: float,\n    index: int,\n) -&gt; jtReal[npt.NDArray, N]\n</code></pre> <p>Central finite difference approximation of a first order derivative.</p> <p>The function evaluates the partial derivative of the function <code>func</code> at the point <code>eval_point</code> for the index <code>index</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Callable to use for FD computation</p> required <code>eval_point</code> <code>npt.NDArray</code> <p>Parameter value at which to approximate the derivative</p> required <code>step_width</code> <code>Real</code> <p>Step width of the finite difference</p> required <code>index</code> <code>int</code> <p>Vector component for which to compute partial derivative</p> required <p>Returns:</p> Type Description <code>jtReal[npt.NDArray, N]</code> <p>npt.NDArray: Partial derivative approximation</p>"},{"location":"api/finitediff/#eikonax.finitediff.finite_diff_2","title":"eikonax.finitediff.finite_diff_2","text":"<pre><code>finite_diff_2(\n    func: Callable[[jtReal[npt.NDArray, M]], jtReal[npt.NDArray, M]],\n    eval_point: jtReal[npt.NDArray, M],\n    step_width: float,\n    index_1: int,\n    index_2: int,\n) -&gt; None\n</code></pre> <p>Implement second order finite differences.</p> <p>Not implemented yet</p>"},{"location":"api/finitediff/#eikonax.finitediff.run_eikonax_with_tensorfield","title":"eikonax.finitediff.run_eikonax_with_tensorfield","text":"<pre><code>run_eikonax_with_tensorfield(\n    parameter_vector: jtReal[npt.NDArray, M],\n    eikonax_solver: solver.Solver,\n    tensor_field: tensorfield.TensorField,\n) -&gt; jtReal[npt.NDArray, N]\n</code></pre> <p>Wrapper function for Eikonax runs.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_vector</code> <code>npt.NDArray</code> <p>Parameter vector at which to compute eikonal solution</p> required <code>eikonax_solver</code> <code>solver.Solver</code> <p>Initialized solver object</p> required <code>tensor_field</code> <code>tensorfield.TensorField</code> <p>Initialized tensor field object</p> required <p>Returns:</p> Type Description <code>jtReal[npt.NDArray, N]</code> <p>npt.NDArray: Solution of the Eikonal equation</p>"},{"location":"api/finitediff/#eikonax.finitediff.compute_fd_jacobian","title":"eikonax.finitediff.compute_fd_jacobian","text":"<pre><code>compute_fd_jacobian(\n    eikonax_solver: solver.Solver,\n    tensor_field: tensorfield.TensorField,\n    stencil: Callable,\n    eval_point: jtReal[npt.NDArray | jax.Array, M],\n    step_width: float,\n) -&gt; jtReal[npt.NDArray, \"N M\"]\n</code></pre> <p>Compute the Jacobian of the Eikonal equation w.r.t. to parameter with finite differences.</p> <p>WARNING: This method should only be used for small problems.</p> <p>Parameters:</p> Name Type Description Default <code>eikonax_solver</code> <code>solver.Solver</code> <p>Initialized solver object</p> required <code>tensor_field</code> <code>tensorfield.TensorField</code> <p>Initialized tensor field object</p> required <code>stencil</code> <code>Callable</code> <p>Finite difference stencil to use for computation</p> required <code>eval_point</code> <code>npt.NDArray</code> <p>Parameter vector at which to approximate derivative</p> required <code>step_width</code> <code>float</code> <p>Step with in FD stencil</p> required <p>Returns:</p> Type Description <code>jtReal[npt.NDArray, 'N M']</code> <p>npt.NDArray: (Dense) Jacobian matrix</p>"},{"location":"api/finitediff/#eikonax.finitediff.compute_fd_hessian","title":"eikonax.finitediff.compute_fd_hessian","text":"<pre><code>compute_fd_hessian(\n    func: Callable,\n    stencil: Callable,\n    eval_point: jtReal[npt.NDArray | jax.Array, M],\n    step_width: float,\n) -&gt; None\n</code></pre> <p>Implement finite difference Hessian computation.</p>"},{"location":"api/logging/","title":"Logging","text":""},{"location":"api/logging/#eikonax.logging","title":"logging","text":"<p>Custom logging, based on Python's built-in logger.</p> <p>Classes:</p> Name Description <code>LoggerSettings</code> <p>Data class for logger settings</p> <code>LogValue</code> <p>Data class for log values</p> <code>Logger</code> <p>Custom logger class</p>"},{"location":"api/logging/#eikonax.logging.LoggerSettings","title":"eikonax.logging.LoggerSettings  <code>dataclass</code>","text":"<p>Logger Settings.</p> <p>Parameters:</p> Name Type Description Default <code>log_to_console</code> <code>bool</code> <p>If True, log messages will be printed to the console.</p> required <code>logfile_path</code> <code>Path</code> <p>Path to the logfile. If None, no logfile will be generated</p> required"},{"location":"api/logging/#eikonax.logging.LogValue","title":"eikonax.logging.LogValue  <code>dataclass</code>","text":"<p>Data class holding info to log.</p> <p>Parameters:</p> Name Type Description Default <code>str_id</code> <code>str</code> <p>String identifier to be shown at the top of the log table</p> required <code>str_format</code> <code>str</code> <p>String format for the log value in log file and on consoles</p> required <code>value</code> <code>Real</code> <p>Actual value to log</p> <code>None</code>"},{"location":"api/logging/#eikonax.logging.Logger","title":"eikonax.logging.Logger","text":"<p>Custom logger class.</p> <p>This class is a minimal wrapper around the Python logger class. It provides handles for logging to the console or a file, depending on the user settings. The class's main interface is the <code>log</code> method, which takes a list of <code>LogValue</code> objects and logs them with the given values and string formats.</p> <p>Methods:</p> Name Description <code>log</code> <p>Log the values in the log_values list</p> <code>header</code> <p>Log the header of the log table</p> <code>info</code> <p>Log a message to the console and/or logfile</p>"},{"location":"api/logging/#eikonax.logging.Logger.__init__","title":"__init__","text":"<pre><code>__init__(logger_settings: LoggerSettings) -&gt; None\n</code></pre> <p>Constructor of the Logger.</p> <p>Sets up log file handlers for printing to console and to file, depending on the user settings.</p> <p>Parameters:</p> Name Type Description Default <code>logger_settings</code> <code>LoggerSettings</code> <p>Settings for initialization, see <code>LoggerSettings</code></p> required"},{"location":"api/logging/#eikonax.logging.Logger.log","title":"log","text":"<pre><code>log(log_values: Iterable[LogValue]) -&gt; None\n</code></pre> <p>Log statistics with given values to initialized file handlers.</p> <p>Parameters:</p> Name Type Description Default <code>log_values</code> <code>Iterable[LogValue]</code> <p>List of log values</p> required"},{"location":"api/logging/#eikonax.logging.Logger.header","title":"header","text":"<pre><code>header(log_values: Iterable[LogValue]) -&gt; None\n</code></pre> <p>Log the table header to console and file.</p> <p>This method should be invoked once at the beginning of the logging process.</p> <p>Parameters:</p> Name Type Description Default <code>log_values</code> <code>Iterable[LogValue]</code> <p>List of log values</p> required"},{"location":"api/logging/#eikonax.logging.Logger.info","title":"info","text":"<pre><code>info(message: str) -&gt; None\n</code></pre> <p>Wrapper to Python logger <code>info</code> call.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Message to log</p> required"},{"location":"api/preprocessing/","title":"Mesh Preprocessing","text":""},{"location":"api/preprocessing/#eikonax.preprocessing","title":"preprocessing","text":"<p>Test mesh creation and preparation for Eikonax solver runs.</p> <p>The creation of test meshes can be done with any other tool. The format of the required adjacency data for Eikonax is strict, however.</p> <p>Functions:</p> Name Description <code>create_test_mesh</code> <p>Create a simple test mesh with Scipy's Delauny functionality.</p> <code>get_adjacent_vertex_data</code> <p>Preprocess mesh data for a vertex-centered evaluation.</p>"},{"location":"api/preprocessing/#eikonax.preprocessing.create_test_mesh","title":"eikonax.preprocessing.create_test_mesh","text":"<pre><code>create_test_mesh(\n    mesh_bounds_x: Annotated[Iterable[float], Is[lambda x: len(x) == 2]],\n    mesh_bounds_y: Annotated[Iterable[float], Is[lambda x: len(x) == 2]],\n    num_points_x: Annotated[int, Is[lambda x: x &gt;= 2]],\n    num_points_y: Annotated[int, Is[lambda x: x &gt;= 2]],\n) -&gt; tuple[jtFloat[npt.NDArray, \"num_vertices dim\"], jtInt[npt.NDArray, \"num_simplices 3\"]]\n</code></pre> <p>Create a simple test mesh with Scipy's Delauny functionality.</p> <p>This methods creates a imple square mesh with Delauny triangulation.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_bounds_x</code> <code>Iterable[float, float]</code> <p>Mesh bounds for x-direction</p> required <code>mesh_bounds_y</code> <code>Iterable[float, float]</code> <p>Mesh bounds for y-direction</p> required <code>num_points_x</code> <code>int</code> <p>Number of vertices for x-direction</p> required <code>num_points_y</code> <code>int</code> <p>Number of vertices for y-direction</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Checks that mesh bounds have correct dimension</p> <code>ValueError</code> <p>Checks that mesh bounds are provided correctly</p> <code>ValueError</code> <p>Checks that at least two mesh points are chosen</p> <p>Returns:</p> Type Description <code>tuple[jtFloat[npt.NDArray, 'num_vertices dim'], jtInt[npt.NDArray, 'num_simplices 3']]</code> <p>tuple[npt.NDArray, npt.NDArray]: Array of vertex coordinates and array of simplex indices</p>"},{"location":"api/preprocessing/#eikonax.preprocessing.get_adjacent_vertex_data","title":"eikonax.preprocessing.get_adjacent_vertex_data","text":"<pre><code>get_adjacent_vertex_data(\n    simplices: jtInt[npt.NDArray, \"num_simplices 3\"],\n    num_vertices: Annotated[int, Is[lambda x: x &gt; 0]],\n) -&gt; jtInt[npt.NDArray, \"num_vertices max_num_adjacent_simplices 4\"]\n</code></pre> <p>Preprocess mesh data for a vertex-centered evaluation.</p> <p>Standard mesh tools provide vertex coordinates and the vertex indices for each simplex. For the vertex-centered solution of the Eikonal equation, however, we need the adjacent simplices/vertices for each vertex. This method performs the necessary transformation.</p> <p>Parameters:</p> Name Type Description Default <code>simplices</code> <code>npt.NDArray</code> <p>Vertex indices for all simplices</p> required <code>num_vertices</code> <code>int</code> <p>Number of vertices in  the mesh</p> required <p>Returns:</p> Type Description <code>jtInt[npt.NDArray, 'num_vertices max_num_adjacent_simplices 4']</code> <p>npt.NDArray: Array containing for each vertex the vertex and simplex indices of all adjacent simplices. Dimension is (num_vertices, max_num_adjacent_simplices, 4), where the 4 entries contain the index of an adjacent simplex and the associated vertices. To ensure homogeneous arrays, all vertices have the same (maximum) number of adjacent simplices. Non-existing simplices are buffered with the value -1.</p>"},{"location":"api/solver/","title":"Forward Solver","text":""},{"location":"api/solver/#eikonax.solver","title":"solver","text":"<p>Eikonax forward solver.</p> <p>Classes:</p> Name Description <code>SolverData</code> <p>Settings for the initialization of the Eikonax Solver.</p> <code>Solution</code> <p>Eikonax solution object, returned by the solver.</p> <code>Solver</code> <p>Eikonax solver class.</p>"},{"location":"api/solver/#eikonax.solver.SolverData","title":"eikonax.solver.SolverData  <code>dataclass</code>","text":"<p>Settings for the initialization of the Eikonax Solver.</p> <p>Parameters:</p> Name Type Description Default <code>loop_type</code> <code>str</code> <p>Type of loop for iterations, options are 'jitted_for', 'jitted_while', 'nonjitted_while'.</p> required <code>max_value</code> <code>Real</code> <p>Maximum value for the initialization of the solution vector.</p> required <code>use_soft_update</code> <code>bool</code> <p>Flag for using soft minmax approximation for optimization parameters</p> required <code>softminmax_order</code> <code>int | None</code> <p>Order of the soft minmax approximation for optimization parameters. Only required if <code>use_soft_update</code> is True.</p> required <code>softminmax_cutoff</code> <code>Real | None</code> <p>Cutoff distance from [0,1] for the soft minmax function. Only required if <code>use_soft_update</code> is True.</p> required <code>max_num_iterations</code> <code>int</code> <p>Maximum number of iterations after which to terminate the solver. Required for all loop types</p> required <code>tolerance</code> <code>Real</code> <p>Absolute difference between iterates in supremum norm, after which to terminate solver. Required for while loop types</p> <code>None</code> <code>log_interval</code> <code>int</code> <p>Iteration interval after which log info is written. Required for non-jitted while loop type.</p> <code>None</code>"},{"location":"api/solver/#eikonax.solver.Solution","title":"eikonax.solver.Solution  <code>dataclass</code>","text":"<p>Eikonax solution object, returned by the solver.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>jax.Array</code> <p>Actual solution vector.</p> required <code>num_iterations</code> <code>int</code> <p>Number of iterations performed in the solve.</p> required <code>tolerance</code> <code>float | jax.Array</code> <p>Tolerance from last two iterates, or entire tolerance history</p> <code>None</code>"},{"location":"api/solver/#eikonax.solver.Solver","title":"eikonax.solver.Solver","text":"<p>               Bases: <code>eqx.Module</code></p> <p>Eikonax solver class.</p> <p>The solver class is the main component for computing the solution of the Eikonal equation for given geometry, tensor field, and initial sites. The Eikonax solver works on the vertex level, meaning that it considers updates from all adjacent triangles to a vertex, instead of all updates for all vertices per triangle. This allows to establish causality in the final solution, which is important for the efficient computation of parametric derivatives. The solver class is mainly a wrapper around different loop constructs, which call vectorized forms of the methods implemented in the <code>corefunctions</code> module. These loop constructs evolve around the loop functionality provided by JAX. Furthermore, the solver class is based on the equinox Module class, which allows for usage of OOP features in JAX.</p> <p>Methods:</p> Name Description <code>run</code> <p>Main interface for Eikonax runs.</p>"},{"location":"api/solver/#eikonax.solver.Solver.__init__","title":"__init__","text":"<pre><code>__init__(\n    mesh_data: corefunctions.MeshData,\n    solver_data: SolverData,\n    initial_sites: corefunctions.InitialSites,\n    logger: logging.Logger | None = None,\n) -&gt; None\n</code></pre> <p>Constructor of the solver class.</p> <p>The constructor initializes all data structures that are re-used in many-query scenarios, such as the solution of inverse problems.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_data</code> <code>corefunctions.MeshData</code> <p>Vertex-based mesh data.</p> required <code>solver_data</code> <code>SolverData</code> <p>Settings for the solver.</p> required <code>initial_sites</code> <code>corefunctions.InitialSites</code> <p>vertices and values for source points</p> required <code>logger</code> <code>logging.Logger | None</code> <p>Logger object, only required for non-jitted while loops. Defaults to None.</p> <code>None</code>"},{"location":"api/solver/#eikonax.solver.Solver.run","title":"run","text":"<pre><code>run(tensor_field: jtFloat[jax.Array | npt.NDArray, 'num_simplices dim dim']) -&gt; Solution\n</code></pre> <p>Main interface for cunducting solver runs.</p> <p>The method initializes the solution vector and dispatches to the run method for the selected loop type.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_field</code> <code>jax.Array</code> <p>Parameter field for which to solve the Eikonal equation. Provides an anisotropy tensor for each simplex of the mesh.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Checks that the chosen loop type is valid.</p> <p>Returns:</p> Name Type Description <code>Solution</code> <code>Solution</code> <p>Eikonax solution object.</p>"},{"location":"api/solver/#eikonax.solver.Solver._run_jitted_for_loop","title":"_run_jitted_for_loop","text":"<pre><code>_run_jitted_for_loop(\n    initial_guess: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_vertices dim dim\"],\n) -&gt; tuple[jtFloat[jax.Array, num_vertices], int, float]\n</code></pre> <p>Solver run with jitted for loop for iterations.</p> <p>The method constructs a JAX-type for loop with fixed number of iterations. For every iteration, a new solution vector is computed from the <code>_compute_global_update</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>initial_guess</code> <code>jax.Array</code> <p>Initial solution vector</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Parameter field</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, num_vertices], int, float]</code> <p>tuple[jax.Array, int, float]: Solution values, number of iterations, tolerance</p>"},{"location":"api/solver/#eikonax.solver.Solver._run_jitted_while_loop","title":"_run_jitted_while_loop","text":"<pre><code>_run_jitted_while_loop(\n    initial_guess: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_vertices dim dim\"],\n) -&gt; tuple[jtFloat[jax.Array, num_vertices], int, float]\n</code></pre> <p>Solver run with jitted while loop for iterations.</p> <p>The iterator is tolerance-based, terminating after a user-defined tolerance for the difference between two consecutive iterates in supremum norm is undercut. For every iteration, a new solution vector is computed from the <code>_compute_global_update</code> method</p> <p>Parameters:</p> Name Type Description Default <code>initial_guess</code> <code>jax.Array</code> <p>Initial solution vector</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Parameter field</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Checks that tolerance has been provided by the user</p> <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, num_vertices], int, float]</code> <p>tuple[jax.Array, int, float]: Solution values, number of iterations, tolerance</p>"},{"location":"api/solver/#eikonax.solver.Solver._run_nonjitted_while_loop","title":"_run_nonjitted_while_loop","text":"<pre><code>_run_nonjitted_while_loop(\n    initial_guess: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_vertices dim dim\"],\n) -&gt; tuple[jtFloat[jax.Array, num_vertices], int, jtFloat[jax.Array, ...]]\n</code></pre> <p>Solver run with standard Python while loop for iterations.</p> <p>While being less performant, the Python while loop allows for logging of infos between iterations. The iterator is tolerance-based, terminating after a user-defined tolerance for the difference between two consecutive iterates in supremum norm is undercut. For every iteration, a new solution vector is computed from the <code>_compute_global_update</code> method</p> <p>Parameters:</p> Name Type Description Default <code>initial_guess</code> <code>jax.Array</code> <p>Initial solution vector</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Parameter field</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Checks that tolerance has been provided by the user</p> <code>ValueError</code> <p>Checks that log interval has been provided by the user</p> <code>ValueError</code> <p>Checks that logger object has been provided by the user</p> <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, num_vertices], int, jtFloat[jax.Array, ...]]</code> <p>tuple[jax.Array, int, int]: Solution values, number of iterations, tolerance vector over all iterations</p>"},{"location":"api/solver/#eikonax.solver.Solver._compute_global_update","title":"_compute_global_update","text":"<pre><code>_compute_global_update(\n    solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_vertices dim dim\"],\n) -&gt; jtFloat[jax.Array, num_vertices]\n</code></pre> <p>Given a current state and tensor field, compute a new solution vector.</p> <p>This method is basically a vectorized call to the <code>_compute_vertex_update</code> method, evaluated over all vertices of the mesh.</p> <p>Parameters:</p> Name Type Description Default <code>solution_vector</code> <code>jax.Array</code> <p>Current state</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Parameter field</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, num_vertices]</code> <p>jax.Array: New iterate</p>"},{"location":"api/solver/#eikonax.solver.Solver._compute_vertex_update","title":"_compute_vertex_update","text":"<pre><code>_compute_vertex_update(\n    old_solution_vector: jtFloat[jax.Array, num_vertices],\n    tensor_field: jtFloat[jax.Array, \"num_vertices dim dim\"],\n    adjacency_data: jtInt[jax.Array, \"max_num_adjacent_simplices 4\"],\n) -&gt; jtFloat[jax.Array, \"\"]\n</code></pre> <p>Compute the update value for a single vertex.</p> <p>This method links to the main logic of the solver routine, based on functions in the <code>corefunctions</code> module.</p> <p>Parameters:</p> Name Type Description Default <code>old_solution_vector</code> <code>jax.Array</code> <p>Current state</p> required <code>tensor_field</code> <code>jax.Array</code> <p>Parameter field</p> required <code>adjacency_data</code> <code>jax.Array</code> <p>Info on all adjacent triangles and respective vertices for the current vertex</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, '']</code> <p>jax.Array: Optimal update value for the current vertex</p>"},{"location":"api/tensorfield/","title":"Parameter Tensor Field","text":""},{"location":"api/tensorfield/#eikonax.tensorfield","title":"tensorfield","text":"<p>Composable and differentiable parameter fields.</p> <p>This module provides ABCs and implementations for the creation of differentiable parameter fields used in Eikonax. To provide sufficient flexibility, the actual tensor field component, implemented in <code>TensorField</code>, comprises two main sub-components. Firstly, we implement a vector-to-simplices map, adhering to the protocol defined in <code>BaseVectorToSimplicesMap</code>. This component maps the global parameter vector to the local parameters of a given simplex. Secondly, we implement a simplex tensor component, adhering to the protocol defined in <code>BaseSimplexTensor</code>. This component assembles the tensor field for a given simplex and a set of parameters for that simplex. The relevant parameters are provided by the <code>VectorToSimplicesMap</code> component from the global parameter vector.</p> <p>Classes:</p> Name Description <code>BaseVectorToSimplicesMap</code> <p>ABC interface contract for vector-to-simplices maps</p> <code>LinearScalarMap</code> <p>Simple one-to-one map from global to simplex parameters</p> <code>BaseSimplexTensor</code> <p>ABC interface contract for assembly of the tensor field</p> <code>LinearScalarSimplexTensor</code> <p>SimplexTensor implementation relying on one parameter per simplex</p> <code>InvLinearScalarSimplexTensor</code> <p>SimplexTensor implementation relying on one parameter per simplex</p> <code>TensorField</code> <p>Tensor field component</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.BaseVectorToSimplicesMap","title":"eikonax.tensorfield.BaseVectorToSimplicesMap","text":"<p>               Bases: <code>ABC</code>, <code>eqx.Module</code></p> <p>ABC interface contract for vector-to-simplices maps.</p> <p>Every component derived from this class needs to implement the <code>map</code> method, which maps returns the relevant parameters for a given simplex from the global parameter vector.</p> <p>Methods:</p> Name Description <code>map</code> <p>Interface for vector-so-simplex mapping</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.BaseVectorToSimplicesMap.map","title":"map  <code>abstractmethod</code>","text":"<pre><code>map(\n    simplex_ind: jtInt[jax.Array, \"\"], parameters: jtReal[jax.Array, num_parameters]\n) -&gt; jtReal[jax.Array, ...]\n</code></pre> <p>Interface for vector-so-simplex mapping.</p> <p>For the given <code>simplex_ind</code>, return those parameters from the global parameter vector that are relevant for the simplex. This methods need to be broadcastable over <code>simplex_ind</code> by JAX (with vmap).</p> <p>Parameters:</p> Name Type Description Default <code>simplex_ind</code> <code>jax.Array</code> <p>Index of the simplex under consideration</p> required <code>parameters</code> <code>jax.Array</code> <p>Global parameter vector</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>ABC error indicating that the method needs to be implemented in subclasses</p> <p>Returns:</p> Type Description <code>jtReal[jax.Array, ...]</code> <p>jax.Array: Relevant parameters for the simplex</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.LinearScalarMap","title":"eikonax.tensorfield.LinearScalarMap","text":"<p>               Bases: <code>BaseVectorToSimplicesMap</code></p> <p>Simple one-to-one map from global to simplex parameters.</p> <p>Every simplex takes exactly one parameter, which is sorted in the global parameter in the same order as the simplices.</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.LinearScalarMap.map","title":"map","text":"<pre><code>map(\n    simplex_ind: jtInt[jax.Array, \"\"], parameters: jtReal[jax.Array, num_parameters_local]\n) -&gt; jtReal[jax.Array, \"\"]\n</code></pre> <p>Return relevant parameters for a given simplex.</p> <p>Parameters:</p> Name Type Description Default <code>simplex_ind</code> <code>jax.Array</code> <p>Index of the simplex under consideration</p> required <code>parameters</code> <code>jax.Array</code> <p>Global parameter vector</p> required <p>Returns:</p> Type Description <code>jtReal[jax.Array, '']</code> <p>jax.Array: relevant parameter (only one)</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.BaseSimplexTensor","title":"eikonax.tensorfield.BaseSimplexTensor","text":"<p>               Bases: <code>ABC</code>, <code>eqx.Module</code></p> <p>ABC interface contract for assembly of the tensor field.</p> <p><code>SimplexTensor</code> components assemble the tensor field for a given simplex and a set of parameters for that simplex. The relevant parameters are provided by the <code>VectorToSimplicesMap</code> component from the global parameter vector. Note that this class provides the metric tensor as used in the inner product for the update stencil of the eikonal equation. This is the INVERSE of the conductivity tensor, which is the actual tensor field in the eikonal equation.</p> <p>Methods:</p> Name Description <code>assemble</code> <p>Assemble the tensor field for a given simplex and parameters</p> <code>derivative</code> <p>Parametric derivative of the <code>assemble</code> method</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.BaseSimplexTensor.__init__","title":"__init__","text":"<pre><code>__init__(dimension: int) -&gt; None\n</code></pre> <p>Constructor, simply fixes the dimension of the tensor field.</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.BaseSimplexTensor.assemble","title":"assemble  <code>abstractmethod</code>","text":"<pre><code>assemble(\n    simplex_ind: jtInt[jax.Array, \"\"], parameters: jtFloat[jax.Array, num_parameters_local]\n) -&gt; jtFloat[jax.Array, \"dim dim\"]\n</code></pre> <p>Assemble the tensor field for given simplex and parameters.</p> <p>Given a parameter array of size n_local, the methods returns a tensor of size dim x dim. The method needs to be broadcastable over <code>simplex_ind</code> by JAX (with vmap).</p> <p>Parameters:</p> Name Type Description Default <code>simplex_ind</code> <code>jax.Array</code> <p>Index of the simplex under consideration</p> required <code>parameters</code> <code>jax.Array</code> <p>Parameters for the simplex</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>ABC error indicating that the method needs to be implemented in subclasses</p> <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'dim dim']</code> <p>jax.Array: Tensor field for the simplex under consideration</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.BaseSimplexTensor.derivative","title":"derivative  <code>abstractmethod</code>","text":"<pre><code>derivative(\n    simplex_ind: jtInt[jax.Array, \"\"], parameters: jtFloat[jax.Array, num_parameters_local]\n) -&gt; jtFloat[jax.Array, \"dim dim num_parameters_local\"]\n</code></pre> <p>Parametric derivative of the <code>assemble</code> method.</p> <p>Given a parameter array of size n_local, the methods returns a Jacobian tensor of size dim x dim x n_local. The method needs to be broadcastable over <code>simplex_ind</code> by JAX (with vmap).</p> <p>Parameters:</p> Name Type Description Default <code>simplex_ind</code> <code>jax.Array</code> <p>Index of the simplex under consideration</p> required <code>parameters</code> <code>jax.Array</code> <p>Parameters for the simplex</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>ABC error indicating that the method needs to be implemented in subclasses</p> <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'dim dim num_parameters_local']</code> <p>jax.Array: Jacobian tensor for the simplex under consideration</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.LinearScalarSimplexTensor","title":"eikonax.tensorfield.LinearScalarSimplexTensor","text":"<p>               Bases: <code>BaseSimplexTensor</code></p> <p>SimplexTensor implementation relying on one parameter per simplex.</p> <p>The simplex tensor is assembled as parameter * Identity for each simplex.</p> <p>Methods:</p> Name Description <code>assemble</code> <p>Assemble the tensor field for a parameter vector</p> <code>derivative</code> <p>Parametric derivative of the <code>assemble</code> method</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.LinearScalarSimplexTensor.assemble","title":"assemble","text":"<pre><code>assemble(\n    _simplex_ind: jtInt[jax.Array, \"\"], parameters: jtFloat[jax.Array, \"\"]\n) -&gt; jtFloat[jax.Array, \"dim dim\"]\n</code></pre> <p>Assemble tensor for given simplex as parameter*Identity.</p> <p>the <code>parameters</code> argument is a scalar here, and <code>_simplex_ind</code> is not used. The method needs to be broadcastable over <code>simplex_ind</code> by JAX (with vmap).</p> <p>Parameters:</p> Name Type Description Default <code>_simplex_ind</code> <code>jax.Array</code> <p>Index of simplex under consideration (not used)</p> required <code>parameters</code> <code>jax.Array</code> <p>Parameter (scalar) for tensor assembly</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'dim dim']</code> <p>jax.Array: Tensor for the simplex</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.LinearScalarSimplexTensor.derivative","title":"derivative","text":"<pre><code>derivative(\n    _simplex_ind: jtInt[jax.Array, \"\"], _parameters: jtFloat[jax.Array, \"\"]\n) -&gt; jtFloat[jax.Array, \"dim dim num_parameters_local\"]\n</code></pre> <p>Parametric derivative of the <code>assemble</code> method.</p> <p>The method needs to be broadcastable over <code>simplex_ind</code> by JAX (with vmap).</p> <p>Parameters:</p> Name Type Description Default <code>_simplex_ind</code> <code>jax.Array</code> <p>Index of simplex under consideration (not used)</p> required <code>_parameters</code> <code>jax.Array</code> <p>Parameter (scalar) for tensor assembly</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'dim dim num_parameters_local']</code> <p>jax.Array: Jacobian tensor for the simplex under consideration</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.InvLinearScalarSimplexTensor","title":"eikonax.tensorfield.InvLinearScalarSimplexTensor","text":"<p>               Bases: <code>BaseSimplexTensor</code></p> <p>SimplexTensor implementation relying on one parameter per simplex.</p> <p>The simplex tensor is assembled as 1/parameter * Identity for each simplex.</p> <p>Methods:</p> Name Description <code>assemble</code> <p>Assemble the tensor field for a parameter vector</p> <code>derivative</code> <p>Parametric derivative of the <code>assemble</code> method</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.InvLinearScalarSimplexTensor.assemble","title":"assemble","text":"<pre><code>assemble(\n    _simplex_ind: jtInt[jax.Array, \"\"], parameters: jtFloat[jax.Array, \"\"]\n) -&gt; jtFloat[jax.Array, \"dim dim\"]\n</code></pre> <p>Assemble tensor for given simplex as 1/parameter*Identity.</p> <p>the <code>parameters</code> argument is a scalar here, and <code>_simplex_ind</code> is not used. The method needs to be broadcastable over <code>simplex_ind</code> by JAX (with vmap).</p> <p>Parameters:</p> Name Type Description Default <code>_simplex_ind</code> <code>jax.Array</code> <p>Index of simplex under consideration (not used)</p> required <code>parameters</code> <code>jax.Array</code> <p>Parameter (scalar) for tensor assembly</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'dim dim']</code> <p>jax.Array: Tensor for the simplex</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.InvLinearScalarSimplexTensor.derivative","title":"derivative","text":"<pre><code>derivative(\n    _simplex_ind: jtInt[jax.Array, \"\"], parameters: jtFloat[jax.Array, \"\"]\n) -&gt; jtFloat[jax.Array, \"dim dim num_parameters_local\"]\n</code></pre> <p>Parametric derivative of the <code>assemble</code> method.</p> <p>The method needs to be broadcastable over <code>simplex_ind</code> by JAX (with vmap).</p> <p>Parameters:</p> Name Type Description Default <code>_simplex_ind</code> <code>jax.Array</code> <p>Index of simplex under consideration (not used)</p> required <code>parameters</code> <code>jax.Array</code> <p>Parameter (scalar) for tensor assembly</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'dim dim num_parameters_local']</code> <p>jax.Array: Jacobian tensor for the simplex under consideration</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.TensorField","title":"eikonax.tensorfield.TensorField","text":"<p>               Bases: <code>eqx.Module</code></p> <p>Tensor field component.</p> <p>Tensor fields combine the functionality of vector-to-simplices maps and simplex tensors according to the composition over inheritance principle. They constitute the full mapping from the global parameter vector to the tensor field over all mesh faces (simplices). In addition, they provide the parametric derivative of that mapping, and assemble the full parameter-to-solution Jacobian of the Eikonax solver from a given partial derivative of the solution vector w.r.t. the tensor field. This introduces some degree of coupling to the eikonax solver, but is the simplest interface for computation of the total derivative according to the chain rule. More detailed explanations are given in the <code>assemble_jacobian</code> method.</p> <p>Methods:</p> Name Description <code>assemble_field</code> <p>Assemble the tensor field for the given parameter vector</p> <code>assemble_jacobian</code> <p>Assemble the parametric derivative of a solution vector for a given parameter vector and derivative of the solution vector w.r.t. the tensor field</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.TensorField.__init__","title":"__init__","text":"<pre><code>__init__(\n    num_simplices: int,\n    vector_to_simplices_map: BaseVectorToSimplicesMap,\n    simplex_tensor: BaseSimplexTensor,\n) -&gt; None\n</code></pre> <p>Constructor.</p> <p>Takes information about the mesh simplices, a vector-to-simplices map, and a simplex tensor map.</p> <p>Parameters:</p> Name Type Description Default <code>num_simplices</code> <code>int</code> <p>Number of simplices in the mesh</p> required <code>vector_to_simplices_map</code> <code>BaseVectorToSimplicesMap</code> <p>Mapping from global to simplex parameters</p> required <code>simplex_tensor</code> <code>BaseSimplexTensor</code> <p>Tensor field assembly for a given simplex</p> required"},{"location":"api/tensorfield/#eikonax.tensorfield.TensorField.assemble_field","title":"assemble_field","text":"<pre><code>assemble_field(\n    parameter_vector: jtFloat[jax.Array | npt.NDArray, num_parameters_global],\n) -&gt; jtFloat[jax.Array, \"num_simplex dim dim\"]\n</code></pre> <p>Assemble global tensor field from global parameter vector.</p> <p>This method simply chains calls to the vector-to-simplices map and the simplex tensor objects, vectorized over all simplices.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_vector</code> <code>jax.Array | npt.NDArray</code> <p>Global parameter vector</p> required <p>Returns:</p> Type Description <code>jtFloat[jax.Array, 'num_simplex dim dim']</code> <p>jax.Array: Global tensor field</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.TensorField.assemble_jacobian","title":"assemble_jacobian","text":"<pre><code>assemble_jacobian(\n    number_of_vertices: int,\n    derivative_solution_tensor: tuple[\n        jtInt[jax.Array, num_values],\n        jtInt[jax.Array, num_values],\n        jtFloat[jax.Array, \"num_values dim dim\"],\n    ],\n    parameter_vector: jtFloat[jax.Array | npt.NDArray, num_parameters_global],\n) -&gt; sp.sparse.coo_matrix\n</code></pre> <p>Assemble partial derivative of the Eikonax solution vector w.r.t. parameters.</p> <p>The total derivative of the solution vector w.r.t. the global parameter vector is given by the chain rule of differentiation. The Eikonax Derivator component evaluates the derivative of the solution vector w.r.t. the tensor field, which is the output of this component. The tensor field assembles the Jacobian tensor of the tensor field w.r.t. to the global parameter vector, and chains it with the solution-tensor derivative in a vectorized form. All computations are done in a sparse matrix format. Consider given a solution-tensor derivative of G_1 of shape N x K x D x D, where N is the number of vertices, K is the number of simplices, and D is the physical dimension of the tensor field. This component internally assembles the tensor-parameter derivative G_2 of shape K x D x D x M, where M is the number of parameters. The total derivative is then computed as a tensor product of G_1 and G_2 over their last and first three dimensions, respectively. The output is a sparse matrix of shape N x M, returned as a scipy COO matrix. The assembly is rather involved, so we handle it internally in this component, a the expense of introducing some additional coupling to the Eikonax Derivator</p> <p>Parameters:</p> Name Type Description Default <code>number_of_vertices</code> <code>int</code> <p>Number of vertices in the mesh</p> required <code>derivative_solution_tensor</code> <code>tuple[jax.Array, jax.Array, jax.Array]</code> <p>Solution-tensor derivative of shape N x K x D x D. Provided as a tuple of row indices, simplex indices, and values, already in sparsified format. The row indices are the indices of the relevant vertices, and can be seen as one half of the index set of the resulting sparse matrix. For each row index, the corresponding simplex index indicates the simplex whose tensor values influence the solution at that vertex by means of the derivative.</p> required <code>parameter_vector</code> <code>jax.Array</code> <p>Global parameter vector</p> required <p>Returns:</p> Type Description <code>sp.sparse.coo_matrix</code> <p>sp.sparse.coo_matrix: Sparse derivative of the Eikonax solution vector w.r.t. the global parameter vector, of shape N x M</p>"},{"location":"api/tensorfield/#eikonax.tensorfield.TensorField._assemble_jacobian","title":"_assemble_jacobian","text":"<pre><code>_assemble_jacobian(\n    simplex_inds: jtFloat[jax.Array, num_values],\n    derivative_solution_tensor_values: jtFloat[jax.Array, num_values],\n    parameter_vector: jtFloat[jax.Array, num_parameters_global],\n) -&gt; tuple[jtFloat[jax.Array, ...], jtInt[jax.Array, ...]]\n</code></pre> <p>Compute the partial derivative of the the tensor field w.r.t. to global parameter vector.</p> <p>Simplex-level derivatives are computed for all provided `simplex_inds' to match the solution-tensor derivatives obtained from the Eikonax derivator.</p> <p>Parameters:</p> Name Type Description Default <code>simplex_inds</code> <code>jax.Array</code> <p>Indices of simplices under consideration</p> required <code>derivative_solution_tensor_values</code> <code>jax.Array</code> <p>Solution-tensor derivative values</p> required <code>parameter_vector</code> <code>jax.Array</code> <p>Global parameter vector</p> required <p>Returns:</p> Type Description <code>tuple[jtFloat[jax.Array, ...], jtInt[jax.Array, ...]]</code> <p>tuple[jax.Array, jax.Array]: Values and column indices of the Jacobian</p>"},{"location":"background/setup/","title":"Software Setup","text":""},{"location":"background/theory/","title":"Theory","text":""},{"location":"usage/derivatives/","title":"Parametric Derivatives","text":""},{"location":"usage/solve/","title":"Forward Solver","text":""}]}